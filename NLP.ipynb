{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import module"
      ],
      "metadata": {
        "id": "Bzx6V1Sz6mHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOEOK_zd4iKK",
        "outputId": "6c06b321-8ff3-43e5-adff-7a40a0486771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import defaultdict\n",
        "from collections import  Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import warnings \n",
        "\n",
        "stop=set(stopwords.words('english'))\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "Zs1q8XaE6p_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "print('Train Data size :{}'.format(train_df.shape))\n",
        "print('Test Data size :{}'.format(test_df.shape))\n",
        "Merge_df = train_df.append(test_df,ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDdNtATi6E3c",
        "outputId": "8220bb61-85b2-48d0-92ed-87153a81c9c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data size :(7613, 5)\n",
            "Test Data size :(3263, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove twitter handle, URL, http tags, punctuation, special characters, numbers"
      ],
      "metadata": {
        "id": "ci-_vWHi9Nc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_pattern(input_txt, pattern):\n",
        "    reg_obj = re.compile(pattern)\n",
        "    output_txt = reg_obj.sub(r'', input_txt)\n",
        "\n",
        "    return output_txt   \n",
        "\n",
        "\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,\"@[\\w]*\"))\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,'https?://\\S+|www\\.\\S+'))\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,'<.*?>'))\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,\"[^a-zA-Z# ]\"))"
      ],
      "metadata": {
        "id": "n-qZbOdu7GU1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stop words"
      ],
      "metadata": {
        "id": "XjH0UW7c99Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text):\n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop] \n",
        "    filtered_tweet = ' '.join(filtered_sentence)\n",
        "    \n",
        "    return filtered_tweet\n",
        "\n",
        "\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_stop_words(x))"
      ],
      "metadata": {
        "id": "0A0uuFjt7Qc1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize & Stemming"
      ],
      "metadata": {
        "id": "BRhV-1uQ-Q44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def tokenize_stem(text):\n",
        "    token_words = word_tokenize(text)\n",
        "    stem_words =[]\n",
        "    for i in token_words:\n",
        "        word = lemmatizer.lemmatize(i)\n",
        "        stem_words.append(word)\n",
        "        \n",
        "    final_tweet = ' '.join(stem_words)\n",
        "    \n",
        "    return final_tweet\n",
        "\n",
        "\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: tokenize_stem(x))"
      ],
      "metadata": {
        "id": "SJt9JwvR-BsK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output cleaned data"
      ],
      "metadata": {
        "id": "1hYhir-KEikd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = Merge_df[:7613]\n",
        "testing_df = Merge_df[7613:]\n",
        "\n",
        "training_df.to_csv(\"train_clean.csv\")\n",
        "testing_df.to_csv(\"test_clean.csv\")"
      ],
      "metadata": {
        "id": "6xciXEMrEhqp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=300, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(Merge_df['text'])\n",
        "\n",
        "train_data  = tfidf[:7613]\n",
        "train_label = Merge_df[:7613]['target']\n",
        "test_data = tfidf[7613:]\n",
        "\n",
        "labels = ['good', 'bad']"
      ],
      "metadata": {
        "id": "rTNsFmre_W3W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB()"
      ],
      "metadata": {
        "id": "XujfRtHGI0cV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "kf = KFold(n_splits=3)\n",
        "\n",
        "test = []\n",
        "pred = []\n",
        "\n",
        "for train_index, test_index in kf.split(train_data):\n",
        "  x_train, x_test = train_data[train_index], train_data[test_index]\n",
        "  y_train, y_test = train_label[train_index], train_label[test_index]\n",
        "  test.extend(y_test)\n",
        "  \n",
        "  model.fit(x_train, y_train)\n",
        "  y_pred = model.predict(x_test)\n",
        "  pred.extend(y_pred)\n",
        "\n",
        "print('Confusion matrix')\n",
        "print(confusion_matrix(test, pred)/3)\n",
        "print(classification_report(test, pred, target_names=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WCvHEZyij1z",
        "outputId": "1b5c2c95-c2d4-4104-e893-28dad258c0c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix\n",
            "[[1137.66666667  309.66666667]\n",
            " [ 460.66666667  629.66666667]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        good       0.71      0.79      0.75      4342\n",
            "         bad       0.67      0.58      0.62      3271\n",
            "\n",
            "    accuracy                           0.70      7613\n",
            "   macro avg       0.69      0.68      0.68      7613\n",
            "weighted avg       0.69      0.70      0.69      7613\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=500)"
      ],
      "metadata": {
        "id": "58fbFYW4KqJF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=3)\n",
        "\n",
        "test = []\n",
        "pred = []\n",
        "\n",
        "for train_index, test_index in kf.split(train_data):\n",
        "  x_train, x_test = train_data[train_index], train_data[test_index]\n",
        "  y_train, y_test = train_label[train_index], train_label[test_index]\n",
        "  test.extend(y_test)\n",
        "  \n",
        "  model.fit(x_train, y_train)\n",
        "  y_pred = model.predict(x_test)\n",
        "  pred.extend(y_pred)\n",
        "\n",
        "print('Confusion matrix')\n",
        "print(confusion_matrix(test, pred)/3)\n",
        "print(classification_report(test, pred, target_names=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-vIUdEjKnUT",
        "outputId": "8a5ba91e-8e4a-4fb3-e819-ca1a6357168f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix\n",
            "[[1120.66666667  326.66666667]\n",
            " [ 474.66666667  615.66666667]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        good       0.70      0.77      0.74      4342\n",
            "         bad       0.65      0.56      0.61      3271\n",
            "\n",
            "    accuracy                           0.68      7613\n",
            "   macro avg       0.68      0.67      0.67      7613\n",
            "weighted avg       0.68      0.68      0.68      7613\n",
            "\n"
          ]
        }
      ]
    }
  ]
}