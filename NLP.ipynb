{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import module"
      ],
      "metadata": {
        "id": "Bzx6V1Sz6mHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOEOK_zd4iKK",
        "outputId": "491d40fb-2d1b-4c58-adee-b311d9a7a9be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import defaultdict\n",
        "from collections import  Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import warnings \n",
        "\n",
        "stop=set(stopwords.words('english'))\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "Zs1q8XaE6p_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "print('Train Data size :{}'.format(train_df.shape))\n",
        "print('Test Data size :{}'.format(test_df.shape))\n",
        "Merge_df = train_df.append(test_df,ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDdNtATi6E3c",
        "outputId": "567dc671-c7e5-4a30-a86b-d3b42e2c2d77"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data size :(7613, 5)\n",
            "Test Data size :(3263, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove twitter handle, URL, http tags, punctuation, special characters, numbers"
      ],
      "metadata": {
        "id": "ci-_vWHi9Nc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_pattern(input_txt, pattern):\n",
        "    reg_obj = re.compile(pattern)\n",
        "    output_txt = reg_obj.sub(r'', input_txt)\n",
        "\n",
        "    return output_txt   \n",
        "\n",
        "\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,\"@[\\w]*\"))\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,'https?://\\S+|www\\.\\S+'))\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,'<.*?>'))\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,\"[^a-zA-Z# ]\"))"
      ],
      "metadata": {
        "id": "n-qZbOdu7GU1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stop words"
      ],
      "metadata": {
        "id": "XjH0UW7c99Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text):\n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop] \n",
        "    filtered_tweet = ' '.join(filtered_sentence)\n",
        "    \n",
        "    return filtered_tweet\n",
        "\n",
        "\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_stop_words(x))"
      ],
      "metadata": {
        "id": "0A0uuFjt7Qc1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize & Stemming"
      ],
      "metadata": {
        "id": "BRhV-1uQ-Q44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def tokenize_stem(text):\n",
        "    token_words = word_tokenize(text)\n",
        "    stem_words =[]\n",
        "    for i in token_words:\n",
        "        word = lemmatizer.lemmatize(i)\n",
        "        stem_words.append(word)\n",
        "        \n",
        "    final_tweet = ' '.join(stem_words)\n",
        "    \n",
        "    return final_tweet\n",
        "\n",
        "\n",
        "Merge_df['text'] = Merge_df['text'].apply(lambda x: tokenize_stem(x))"
      ],
      "metadata": {
        "id": "SJt9JwvR-BsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output cleaned data"
      ],
      "metadata": {
        "id": "1hYhir-KEikd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = Merge_df[:7613]\n",
        "testing_df = Merge_df[7613:]\n",
        "\n",
        "training_df.to_csv(\"train_clean.csv\")\n",
        "testing_df.to_csv(\"test_clean.csv\")"
      ],
      "metadata": {
        "id": "6xciXEMrEhqp"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=300, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(Merge_df['text'])\n",
        "\n",
        "train_data  = tfidf[:7613]\n",
        "test_data = tfidf[7613:]\n",
        "print(tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTNsFmre_W3W",
        "outputId": "8867c024-cfd1-4bff-9f18-2c81adaf6424"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10876, 300)\n"
          ]
        }
      ]
    }
  ]
}